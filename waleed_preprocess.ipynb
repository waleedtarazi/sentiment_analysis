{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "af6058ed",
      "metadata": {
        "id": "af6058ed"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import collections \n",
        "import nltk\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6d7d8426",
      "metadata": {},
      "outputs": [],
      "source": [
        "arb_stop_words = set(nltk.corpus.stopwords.words(\"arabic\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e76bdf5a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def label_standrlization(word):\n",
        "    lower_word = word.lower()\n",
        "    if lower_word.startswith('p'):\n",
        "        return 'positive'\n",
        "    if lower_word.startswith('irr'):\n",
        "        return 'irrelevante'\n",
        "    if lower_word.endswith('al'):\n",
        "        return 'neutral'\n",
        "    if lower_word.startswith('neg'):\n",
        "        return 'negative'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "746467ec",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "91dc2107",
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_nan(df):\n",
        "    Nan_columns = df.columns[df.isnull().any()]\n",
        "    if len(Nan_columns) > 0:\n",
        "        for i in Nan_columns:\n",
        "                print(i, df[i].isnull().sum())\n",
        "    else : \n",
        "        print(\" No null values found üòç\")        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c8045b0d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def delete_nan_duplicates(df):\n",
        "    df = df.replace(r'^\\s*$',float('NaN'),regex =True)\n",
        "    df.drop_duplicates(inplace = True)\n",
        "    df.dropna(inplace = True)\n",
        "    return df\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a9912c53",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        " \n",
        "# import numpy module\n",
        "import numpy as np\n",
        " \n",
        "# create dataframe with 3 columns\n",
        "data = pd.DataFrame({\n",
        " \n",
        "    \"name\": ['sravan', np.nan, 'harsha', 'ramya'],\n",
        "    \"subjects\": [np.nan, 'java', np.nan, 'html/php'],\n",
        "    \"marks\": [98, np.nan, np.nan, np.nan]\n",
        "})\n",
        "\n",
        "data.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "63f0551d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def toknize_tweet(tweet):\n",
        "    tweet_tokenizer = nltk.TweetTokenizer()\n",
        "    tweet_tokens = tweet_tokenizer.tokenize(tweet)\n",
        "    tweet_clean = []\n",
        "    for token in tweet_tokens:\n",
        "        tweet_clean.append(token)\n",
        "    return tweet_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "19a8d72f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def toknize_df(df,text_label,tokinze_label):\n",
        "    df[tokinze_label] = df[text_label].apply(lambda row: toknize_tweet(row))\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "bad03421",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_tokens_list(df,column):\n",
        "    list_tokinz = [word for sample in column for word in sample]\n",
        "    return  list_tokinz , len(list_tokinz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a7582a5b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_tokens_list_set(list):\n",
        "    temp = set(list)\n",
        "    return temp, len(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3WFp0g3DQ72V",
      "metadata": {
        "id": "3WFp0g3DQ72V"
      },
      "outputs": [],
      "source": [
        "def most_frequent(text,num):\n",
        "    fdist = nltk.FreqDist(text)\n",
        "    return fdist.most_common(num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c8aca60d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def least_frequent_words(text):\n",
        "    fdist = nltk.FreqDist(text)\n",
        "    return fdist.hapaxes()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "0c19fba2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def N_grams_freq(text,N,num):\n",
        "    return most_frequent(nltk.ngrams(text, N),10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "52afd8c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def most_occure_collications(text,window):\n",
        "    return nltk.Text(text).collocations(10,window)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "926c6d81",
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.collocations import *\n",
        "\n",
        "def apply_gram_collication(text):\n",
        "    #\n",
        "        bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "        finder = BigramCollocationFinder.from_words(text)\n",
        "        finder.apply_freq_filter(3)\n",
        "        print('\\n the most Bigram',finder.nbest(bigram_measures.likelihood_ratio, 10))\n",
        "    # \n",
        "        trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
        "        finder = TrigramCollocationFinder.from_words(text)\n",
        "        finder.apply_freq_filter(3)\n",
        "        print('\\n the most Trigram',finder.nbest(trigram_measures.likelihood_ratio, 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ba92ab51",
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_all_information(text,data_type):\n",
        "    print('the MOST commen WORDS in',data_type,'are:\\n',most_frequent(text,15))\n",
        "    print('\\nthe LEAST commen WORDS',data_type,' are:\\n',least_frequent_words(text))\n",
        "    print('\\nthe MOST commen 2_grams_freq',data_type,' are:\\n',N_grams_freq(text,2,10))\n",
        "    print('\\nthe MOST commen 3_grams_freq',data_type,' are:\\n',N_grams_freq(text,3,10))\n",
        "\n",
        "    \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6fd603ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_hash(x):\n",
        "     for word in x:\n",
        "        if word.startswith('#'):\n",
        "            return x\n",
        "        else:\n",
        "            return ''\n",
        "def most_hashes(text):\n",
        "    data_hash_none =[ find_hash(x) for x in text]\n",
        "    data_hash = [x for x in data_hash_none if x != '']\n",
        "    frq_hash = nltk.FreqDist(data_hash)\n",
        "    return frq_hash.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "0bfb543a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_number_of_tweet(column):\n",
        "    x = [len(tweet) for tweet in column]\n",
        "    plt.ylabel('number of tweets')\n",
        "    plt.xlabel('Lenght(from tweets)')\n",
        "    plt.hist(x,bins=60)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "LhllhojDRNZd",
      "metadata": {
        "id": "LhllhojDRNZd"
      },
      "outputs": [],
      "source": [
        "def delete_url(text):\n",
        "    return re.sub(r'http\\S*',' ', text)\n",
        "\n",
        "def delete_mention(text):\n",
        "    return re.sub('@\\w*',' ',text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "6a6e62a5",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'@ZAINABKADYROV ÿßŸÑŸÑŸÑŸá ŸäŸÉŸàŸÜ ŸÖÿπ ÿ±Ÿàÿ≥Ÿäÿß ŸÖŸÜ ÿ´ÿπÿßŸÑÿ® ÿßŸÑÿ£ÿ±ÿ∂'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testing = pd.read_csv('train.csv')\n",
        "testing['text'][2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "3c4a5f24",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'  ÿßŸÑŸÑŸÑŸá ŸäŸÉŸàŸÜ ŸÖÿπ ÿ±Ÿàÿ≥Ÿäÿß ŸÖŸÜ ÿ´ÿπÿßŸÑÿ® ÿßŸÑÿ£ÿ±ÿ∂'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "delete_mention(testing['text'][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "M_bO-W2DRNZe",
      "metadata": {
        "id": "M_bO-W2DRNZe"
      },
      "outputs": [],
      "source": [
        "def delete_hashtags(text):\n",
        "    return re.sub('#\\w*_*' , '',text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "l-5TSuHWRNZe",
      "metadata": {
        "id": "l-5TSuHWRNZe"
      },
      "outputs": [],
      "source": [
        "def delete_repeated(text):\n",
        "    pattern = r'(.)\\1{3,}'\n",
        "    output = re.sub(pattern,r'\\1', text)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e77927b8",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "ohGu5R14RNZe",
      "metadata": {
        "id": "ohGu5R14RNZe"
      },
      "outputs": [],
      "source": [
        "arabic_numbers  = r'Ÿ°Ÿ¢Ÿ£Ÿ§Ÿ•Ÿ¶ŸßŸ®Ÿ©Ÿ†'\n",
        "english_numbers = r'1234567890'\n",
        "arabic_regexp = '[Ÿ°Ÿ¢Ÿ£Ÿ§Ÿ•Ÿ¶ŸßŸ®Ÿ©Ÿ†]'\n",
        "def _sub(match_object, digits):\n",
        "    return english_numbers[digits.find(match_object.group(0))]\n",
        "def _sub_arabic(match_object):\n",
        "    return _sub(match_object, arabic_numbers)\n",
        "def replace_arabic(match_obj):\n",
        "    return re.sub(arabic_regexp, _sub_arabic, match_obj.group())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "0848cc02",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def switch_numbers(text):\n",
        "    arabic_regx ='[Ÿ°Ÿ¢Ÿ£Ÿ§Ÿ•Ÿ¶ŸßŸ®Ÿ©Ÿ†]'\n",
        "    return re.sub(arabic_regx,replace_arabic,text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "50d3ff39",
      "metadata": {},
      "outputs": [],
      "source": [
        "def numbers_to_tag(text):\n",
        "    return re.sub(r'\\d','<NUM>',text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "QJX_PtSzSAL6",
      "metadata": {
        "id": "QJX_PtSzSAL6"
      },
      "outputs": [],
      "source": [
        "def delete_numbers(text):\n",
        "    return re.sub(r'\\d','',text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "caLTQ8EtSRhT",
      "metadata": {
        "id": "caLTQ8EtSRhT"
      },
      "outputs": [],
      "source": [
        "def delete_pun(text):\n",
        "    my_punctt = ['!', '\"', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '.',\n",
        "           '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', \n",
        "           '`', '{', '|', '}', '~', '¬ª','¬´', '‚Äú', '‚Äù']\n",
        "    pattern = re.compile(\"[\"+\"\".join(my_punctt)+\"]\")\n",
        "    text= re.sub(pattern,'',text)\n",
        "    text = re.sub(r'#\\s|_\\s' ,\" \", text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "4439b9a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "my_punct_ = ['@' , '#' , '_']\n",
        "emojis =[\n",
        "    \"\\U00002600-\\U000027BF\",\n",
        "    \"\\U0001f300-\\U0001f64F\",\n",
        "    \"\\U0001F1E0-\\U0001F1FF\", # flags (iOS)\n",
        "    \"\\U0001F300-\\U0001F5FF\",  # symbols & pictographs\n",
        "    \"\\U0001F600-\\U0001F64F\",  # emoticons\n",
        "    \"\\U0001F680-\\U0001F6FF\", # transport & map symbols\n",
        "    \"\\U0001F700-\\U0001F77F\", # alchemical symbols\n",
        "    \"\\U0001F780-\\U0001F7FF\", # Geometric Shapes Extended\n",
        "    \"\\U0001F800-\\U0001F8FF\", # Supplemental Arrows-C\n",
        "    \"\\U0001F900-\\U0001F9FF\", # Supplemental Symbols and Pictographs\n",
        "    \"\\U0001FA00-\\U0001FA6F\", # Chess Symbols\n",
        "    \"\\U0001FA70-\\U0001FAFF\",\n",
        "    \"\\U0001F1E6-\\U0001F1FF\",\n",
        "    \"\\U000024C2-\\U0001F251\",\n",
        "    \"\\U0001f926-\\U0001f937\",\n",
        "    \"\\U0001F620\",\n",
        "    \"\\u200d\",\n",
        "    \"\\u2640-\\u2642\",\n",
        "    \"\\U0001F1F2-\\U0001F1F4\",  # Symbols and Pictographs Extended-A\n",
        "    \"\\U00002702-\\U000027B0\", # Dingbats # Dingbats\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "UZuYtSHaSRhU",
      "metadata": {
        "id": "UZuYtSHaSRhU"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' ŸÑŸÑÿ¢ŸÜ ÿ•ŸÜ ÿ° ŸàŸÑŸä  ÿ£Ÿä ÿ¢ŸÑ ÿßÿ¶ ÿ≠ÿ®Ÿá'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def delete_emojies(text):\n",
        "  emoji_patt = re.compile(\n",
        "    \"([\"\n",
        "    \"\\U00002600-\\U000027BF\"\n",
        "    \"\\U0001f300-\\U0001f64F\"\n",
        "    \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "    \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "    \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "    \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "    \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
        "    \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
        "    \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
        "    \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
        "    \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
        "    \"\\U0001FA70-\\U0001FAFF\"\n",
        "    \"\\U0001F1E6-\\U0001F1FF\"\n",
        "    \"\\U000024C2-\\U0001F251\"\n",
        "    \"\\U0001f926-\\U0001f937\"\n",
        "    \"\\U0001F620\"\n",
        "    \"\\u200d\"\n",
        "    \"\\u2640-\\u2642\"\n",
        "    \"\\U0001F1F2-\\U0001F1F4\"  # Symbols and Pictographs Extended-A\n",
        "    \"\\U00002702-\\U000027B0\"  # Dingbats\n",
        "    \"])\"\n",
        "  )\n",
        "  return re.sub(emoji_patt,' ', text)\n",
        "delete_emojies(\" ŸÑŸÑÿ¢ŸÜ ÿ•ŸÜ ÿ° ŸàŸÑŸä ‚ù§ÿ£Ÿä ÿ¢ŸÑüíùÿßÿ¶ ÿ≠ÿ®Ÿá\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "otquvQjuSRhV",
      "metadata": {
        "id": "otquvQjuSRhV"
      },
      "outputs": [],
      "source": [
        "def delete_non_arabi(text):\n",
        "    punc_emoj = my_punct_ + emojis\n",
        "    emojis_pun_str=''\n",
        "    for i in emojis:\n",
        "        emojis_pun_str +=i\n",
        "    all_patterens_str = emojis_pun_str +\"\".join(punc_emoj)+'Ÿ°Ÿ¢Ÿ£Ÿ§Ÿ•Ÿ¶ŸßŸ®Ÿ©Ÿ†'\n",
        "    all_patterens_regx = re.compile(\"[^\"+all_patterens_str+\"0-9\"+ \"ÿ°-Ÿä\"+\"]\")\n",
        "    return re.sub(all_patterens_regx, r' ',text) # the \"-\" symbol should remain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "B6icsYsFRNZf",
      "metadata": {
        "id": "B6icsYsFRNZf"
      },
      "outputs": [],
      "source": [
        "def delete_stop_words(text):\n",
        "    tokens = text.split()\n",
        "    return \" \".join([w for w in tokens if not w in arb_stop_words])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "16d556a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "st = ISRIStemmer()\n",
        "def steaming_text(text):\n",
        "    textlist = text.split()\n",
        "    stemmed = []\n",
        "    for word in textlist:\n",
        "        stemmed.append(st.stem(word))\n",
        "    return \" \".join(stemmed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "wk-Z3Gw4Sd3O",
      "metadata": {
        "id": "wk-Z3Gw4Sd3O"
      },
      "outputs": [],
      "source": [
        "def hamza_standardization(text):\n",
        "   return re.sub(r\"[ÿ§ÿ¶ÿ°]\", \"ÿ°\", text)     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "bDLOsd4tSd3O",
      "metadata": {
        "id": "bDLOsd4tSd3O"
      },
      "outputs": [],
      "source": [
        "def aliphs_standardization(text):\n",
        "    return re.sub(r\"[ÿ•ÿ£Ÿ±ÿ¢ÿß]\", \"ÿß\", text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "JhSOCYLmSd3P",
      "metadata": {
        "id": "JhSOCYLmSd3P"
      },
      "outputs": [],
      "source": [
        "def tatwil_remover(text):\n",
        "    return re.sub(r'ŸÄ' , \"\",text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "wWJ-Z11_Sls-",
      "metadata": {
        "id": "wWJ-Z11_Sls-"
      },
      "outputs": [],
      "source": [
        "def tanwin_removal(text):\n",
        "    return re.sub(r'[ Ÿé Ÿã Ÿê Ÿç Ÿè Ÿå Ÿë Ÿí]',' ',text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "AkVzG3AWVgZA",
      "metadata": {
        "id": "AkVzG3AWVgZA"
      },
      "outputs": [],
      "source": [
        "def white_space_normlization(text):\n",
        "    return re.sub(r'\\s+',\" \",text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "pCJ7itd3tJjw",
      "metadata": {
        "id": "pCJ7itd3tJjw"
      },
      "outputs": [],
      "source": [
        "def delete_duplicates_rows(df):\n",
        "    return df.drop_duplicates(inplace = True, ignore_index = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "1747f25c",
      "metadata": {
        "id": "1747f25c"
      },
      "outputs": [],
      "source": [
        "def preprocess_tweet(tweet,\n",
        " delete_urls,\n",
        " delete_mentions,\n",
        " delete_hashtagss,\n",
        " delete_repeateds,\n",
        " switch_numberss,\n",
        " numbers_to_tags,\n",
        " delete_numberss,\n",
        " delete_puns,\n",
        " delete_emojiess,\n",
        " delete_non_arabis,\n",
        " delete_stop_wordss,\n",
        " steaming_texts,\n",
        " hamza_standardizations,\n",
        " aliphs_standardizations,\n",
        " tatwil_removers,\n",
        " tanwin_removals,\n",
        " delete_white_spaces,\n",
        " ):\n",
        "    \"\"\"Process tweet function.\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "        flags: list of \"work\" values for all functions will called here.\n",
        "    Output:\n",
        "        cleaned_tweet: tweet after apply all cleaning and normlizaing functions\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if delete_urls:\n",
        "        tweet = delete_url(tweet)\n",
        "\n",
        "    if delete_mentions:\n",
        "        tweet = delete_mention(tweet)\n",
        "\n",
        "    if delete_repeateds:\n",
        "        tweet = delete_repeated(tweet)\n",
        "\n",
        "    if switch_numberss:\n",
        "        tweet = switch_numbers(tweet)\n",
        "\n",
        "    if delete_numberss:\n",
        "        tweet = delete_numbers(tweet)\n",
        "\n",
        "    if numbers_to_tags:\n",
        "        tweet = numbers_to_tag(tweet)\n",
        "\n",
        "\n",
        "    if delete_hashtagss:\n",
        "        tweet = delete_hashtags(tweet)\n",
        "\n",
        "\n",
        "    if delete_non_arabis:\n",
        "        tweet = delete_non_arabi(tweet)\n",
        "\n",
        "\n",
        "    if delete_puns:\n",
        "        tweet = delete_pun(tweet)\n",
        "\n",
        "\n",
        "    if delete_emojiess:\n",
        "        tweet = delete_emojies(tweet)\n",
        "\n",
        "    if delete_stop_wordss:\n",
        "        tweet = delete_stop_words(tweet)\n",
        "\n",
        "\n",
        "    if hamza_standardizations:\n",
        "        tweet = hamza_standardization(tweet)\n",
        "\n",
        "    if aliphs_standardizations:\n",
        "        tweet = aliphs_standardization(tweet)\n",
        "\n",
        "    if tatwil_removers:\n",
        "        tweet = tatwil_remover(tweet)\n",
        "\n",
        "    if tanwin_removals:\n",
        "        tweet = tanwin_removal(tweet)\n",
        "\n",
        "    if steaming_texts:\n",
        "        tweet = steaming_text(tweet)\n",
        "\n",
        "    if delete_white_spaces:\n",
        "        tweet = white_space_normlization(tweet)\n",
        "\n",
        "    return tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "4c12639b",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def preprocesse_df(df,field,**kwargs):\n",
        "    # for parm in kwargs:\n",
        "    parms = [\n",
        "        'delete_mentions',\n",
        "        'delete_urls'\n",
        "        'delete_puns',\n",
        "        'delete_hashtagss',\n",
        "        'delete_repeateds',\n",
        "        'switch_numberss',\n",
        "        'tatwil_removers',\n",
        "        'delete_stop_wordss',\n",
        "        'delete_white_spaces',\n",
        "        'numbers_to_tags',\n",
        "        'delete_numberss',\n",
        "        'delete_emojiess',\n",
        "        'delete_non_arabis',\n",
        "        'hamza_standardizations',\n",
        "        'steaming_texts',\n",
        "        'aliphs_standardizations',\n",
        "        'tanwin_removals',]\n",
        "\n",
        "    for parm in parms:\n",
        "        kwargs.setdefault(parm,False)\n",
        "\n",
        "    df[field] = df[field].apply( lambda row : preprocess_tweet(str(row),\n",
        "    delete_mentions= kwargs.get('delete_mentions'),\n",
        "    delete_urls = kwargs.get('delete_urls'),\n",
        "    delete_puns = kwargs.get('delete_puns'),\n",
        "    delete_hashtagss = kwargs.get('delete_hashtagss'),\n",
        "    delete_repeateds = kwargs.get('delete_repeateds'),\n",
        "    switch_numberss = kwargs.get('switch_numberss'),\n",
        "    tatwil_removers = kwargs.get('tatwil_removers'),\n",
        "    delete_stop_wordss = kwargs.get('delete_stop_wordss'),\n",
        "    delete_white_spaces = kwargs.get('delete_white_spaces'),\n",
        "    numbers_to_tags = kwargs.get('numbers_to_tags'),\n",
        "    delete_numberss = kwargs.get('delete_numberss'),\n",
        "    delete_emojiess = kwargs.get('delete_emojiess'),\n",
        "    delete_non_arabis = kwargs.get('delete_non_arabis'),\n",
        "    hamza_standardizations = kwargs.get('hamza_standardizations'),\n",
        "    steaming_texts = kwargs.get('steaming_texts'),\n",
        "    aliphs_standardizations = kwargs.get('aliphs_standardizations'),\n",
        "    tanwin_removals = kwargs.get('tanwin_removals'),\n",
        "    ))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "821dd1c7",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "KFerFDFqPwPI",
        "-4LEgCtvPwaQ",
        "ZgzDVRqCQ72Q",
        "qQlGbCI3Q77R",
        "hIwnQVuas5hE",
        "7orTRN5vs9vp",
        "cwaCNfS9RNZc",
        "MbJeF_CxRNZd",
        "GY-HhD2YRNZe",
        "KV9D86kbRNZe",
        "3fM_FPldSCxp",
        "N2z1KmQXSAFs",
        "fCBHc2y6SAL5",
        "P1aLjPbzRNZe",
        "dbqN7BceSRhS",
        "rad8wPeSSRhU",
        "FbIUIkezSRhU",
        "N4WDktTgRNZf",
        "wbGPWNtoR4p-",
        "V-rCzzoMR41T",
        "iXF2L4Z-Sd3N",
        "ij5gdJA4Sd3O",
        "H6k8JnJDSd3P",
        "1TmDb343Sls9",
        "0V5LD9EWVgY7",
        "vc96CuQStFA9",
        "exdhuH6GtLez",
        "pwtVy8DES8AZ",
        "npObP2JLUBgw",
        "pTIrTECWUBg5",
        "6v0njm5qUBg6"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "f944705f064e68791190f4b1fe538986b5ceff64e1da9d2f83c5f90d017b2289"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
